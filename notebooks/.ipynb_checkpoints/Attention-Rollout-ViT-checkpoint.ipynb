{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "from experiment import OCTClassification\n",
    "from nntools.utils import Config\n",
    "sys.path.append('../Transformer-Explainability')\n",
    "from baselines.ViT.ViT_LRP import VisionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_savedict = '/home/clement/Documents/Clement/runs/mlruns/1/505896c1301b4f6790efd15dca57b8f9/artifacts/iteration_25000_mIoU_0920.pth'\n",
    "\n",
    "state_dict = torch.load(trained_savedict)\n",
    "state_dict = state_dict['model_state_dict']\n",
    "reformated_state_dict = {}\n",
    "for k in state_dict:\n",
    "    reformated_state_dict[k.split('network.')[1]] = state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import vit_base_patch32_384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.ViT.ViT_explanation_generator import LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(patch_size=32, embed_dim=768, depth=12, \n",
    "                          num_heads=12, img_size=384, num_classes=4, qkv_bias=True)\n",
    "model.load_state_dict(reformated_state_dict, strict=True)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "attribution_generator = LRP(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'OCT-Classification'. Detailed error Yaml file '/home/clement/Documents/Clement/runs/OCT-Classification/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 237, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 311, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/utils/file_utils.py\", line 170, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/clement/Documents/Clement/runs/OCT-Classification/meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment 'FundusClassification'. Detailed error Yaml file '/home/clement/Documents/Clement/runs/FundusClassification/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 237, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 311, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/utils/file_utils.py\", line 170, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/clement/Documents/Clement/runs/FundusClassification/meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment 'server'. Detailed error Yaml file '/home/clement/Documents/Clement/runs/server/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 237, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 311, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/utils/file_utils.py\", line 170, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/clement/Documents/Clement/runs/server/meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/home/clement/Documents/Clement/runs/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 237, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 311, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/clement/anaconda3/envs/py37/lib/python3.7/site-packages/mlflow/utils/file_utils.py\", line 170, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/clement/Documents/Clement/runs/mlruns/meta.yaml' does not exist.\n"
     ]
    }
   ],
   "source": [
    "config_path = '../config.yaml'\n",
    "config = Config(config_path)\n",
    "experiment = OCTClassification(config)\n",
    "dataset = experiment.test_dataset\n",
    "labels = {} \n",
    "for k, v in dataset.map_class.items():\n",
    "    labels[v] = k\n",
    "\n",
    "def show_cam_on_image(img, mask, alpha):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap*alpha + (1-alpha)*np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "\n",
    "\n",
    "def generate_visualization(original_image, class_index=None, alpha=0.5):\n",
    "    transformer_attribution = attribution_generator.generate_LRP(original_image, method=\"transformer_attribution\", index=class_index).detach()\n",
    "    transformer_attribution = transformer_attribution.reshape(1, 1, 12, 12)\n",
    "    transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, scale_factor=32, mode='bilinear')\n",
    "    transformer_attribution = transformer_attribution.cuda().data.cpu().numpy()\n",
    "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    \n",
    "    transformer_attribution = np.squeeze(transformer_attribution)\n",
    "    image_transformer_attribution = original_image.squeeze(0).permute(1, 2, 0).data.cpu().numpy()\n",
    "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
    "    vis = show_cam_on_image(image_transformer_attribution, transformer_attribution, alpha)\n",
    "    vis =  np.uint8(255 * vis)\n",
    "#     vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    return image_transformer_attribution, vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "gts = []\n",
    "index = np.arange(1000)\n",
    "np.random.shuffle(index)    \n",
    "output_vit = 'Explained-ViT-LRP/'\n",
    "output_img = 'KermanyTestSet/'\n",
    "\n",
    "if not os.path.exists(output_vit):\n",
    "    os.makedirs(output_vit)\n",
    "\n",
    "if not os.path.exists(output_img):\n",
    "    os.makedirs(output_img)\n",
    "    \n",
    "def plot():\n",
    "    for i in index:\n",
    "        img, gt = dataset[i]\n",
    "        filename = dataset.filename(i)\n",
    "        img = img.unsqueeze(0).cuda()\n",
    "        pred = model(img)\n",
    "        prob = torch.softmax(pred, dim=1)\n",
    "        pred = torch.argmax(prob, dim=1)\n",
    "        argsort = torch.argsort(prob, dim=1, descending=True)\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)    \n",
    "        inp, out = generate_visualization(img, pred)\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(9,9)\n",
    "        ax.grid(False)\n",
    "        # Hide axes ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(inp)\n",
    "        ax.set_title('Groundtruth: '+ labels[int(gt)] )\n",
    "        plt.tight_layout()    \n",
    "        plt.savefig(os.path.join(output_img,filename))    \n",
    "\n",
    "        plt.close(fig)\n",
    "        title = ''\n",
    "        for arg in argsort.squeeze(0)[:2]:\n",
    "            title = title + ' ' + labels[int(arg)]+', Pr: {:.2%}'.format((prob[0][arg]))\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.grid(False)\n",
    "        # Hide axes ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        fig.set_size_inches(9,9)\n",
    "\n",
    "        ax.set_title('Predicted: '+ title)\n",
    "        ax.imshow(out)\n",
    "        plt.tight_layout()    \n",
    "    #     plt.savefig(os.path.join(output_vit,filename))\n",
    "    #     plt.close(fig)\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "def explain_map():\n",
    "    for i in index:\n",
    "        img, gt = dataset[i]\n",
    "        filename = dataset.filename(i)\n",
    "        img = img.unsqueeze(0).cuda()\n",
    "        pred = model(img)\n",
    "        prob = torch.softmax(pred, dim=1)\n",
    "        pred = torch.argmax(prob, dim=1)\n",
    "        argsort = torch.argsort(prob, dim=1, descending=True)\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)    \n",
    "        inp, out = generate_visualization(img, pred, alpha=1)\n",
    "        \n",
    "#         cv2.imwrite(os.path.join(output_vit, filename))\n",
    "        filepath = dataset.img_filepath[i] \n",
    "        inp = cv2.imread(filepath)\n",
    "        \n",
    "        out = cv2.resize(out, dsize=inp.shape[:2][::-1])\n",
    "        \n",
    "        cv2.imwrite(os.path.join(output_vit, filename), out)\n",
    "\n",
    "        \n",
    "explain_map()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "gts = np.asarray([int(_) for _ in gts])\n",
    "preds = np.asarray([int(_) for _ in preds ])\n",
    "print(np.sum(gts==preds)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

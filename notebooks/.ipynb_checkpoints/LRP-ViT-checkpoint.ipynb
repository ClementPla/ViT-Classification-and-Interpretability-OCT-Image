{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "from experiment import OCTClassification\n",
    "sys.path.append('../Transformer-Explainability')\n",
    "from baselines.ViT.ViT_LRP import VisionTransformer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_savedict = '/home/clement/Documents/Clement/runs/mlruns/1/8cde2d9e96fb475681128bce91f90c07/artifacts/iteration_100000_mIoU_0901.pth'\n",
    "\n",
    "state_dict = torch.load(trained_savedict)\n",
    "state_dict = state_dict['model_state_dict']\n",
    "reformated_state_dict = {}\n",
    "for k in state_dict:\n",
    "    reformated_state_dict[k.split('network.')[1]] = state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import vit_base_patch32_384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.ViT.ViT_explanation_generator import LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(patch_size=32, embed_dim=768, depth=12, \n",
    "                          num_heads=12, img_size=384, num_classes=4, qkv_bias=True)\n",
    "model.load_state_dict(reformated_state_dict, strict=True)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "attribution_generator = LRP(model)\n",
    "import cv2\n",
    "import numpy as np\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap*0.25 + 0.75*np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '../config.yaml'\n",
    "from nntools.utils import load_yaml\n",
    "config = load_yaml(config_path)\n",
    "experiment = OCTClassification(config)\n",
    "dataset = experiment.test_dataset\n",
    "\n",
    "\n",
    "def generate_visualization(original_image, class_index=None):\n",
    "    transformer_attribution = attribution_generator.generate_LRP(original_image, method=\"transformer_attribution\", index=class_index).detach()\n",
    "    transformer_attribution = transformer_attribution.reshape(1, 1, 12, 12)\n",
    "    transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, scale_factor=32, mode='bilinear')\n",
    "    transformer_attribution = transformer_attribution.cuda().data.cpu().numpy()\n",
    "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    \n",
    "    transformer_attribution = np.squeeze(transformer_attribution)\n",
    "    image_transformer_attribution = original_image.squeeze(0).permute(1, 2, 0).data.cpu().numpy()\n",
    "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
    "    vis = show_cam_on_image(image_transformer_attribution, transformer_attribution)\n",
    "    vis =  np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    return image_transformer_attribution, vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "gts = []\n",
    "index = np.arange(1000)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "\n",
    "labels = {} \n",
    "for k, v in dataset.map_class.items():\n",
    "    labels[v] = k\n",
    "    \n",
    "import os\n",
    "output = 'ViT-LRP-Explained_Kermany/'\n",
    "\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "for i in index:\n",
    "    img, gt = dataset[i]\n",
    "    filename = dataset.filename(i)\n",
    "    img = img.unsqueeze(0).cuda()\n",
    "    pred = model(img)\n",
    "    prob = torch.softmax(pred, dim=1)\n",
    "    pred = torch.argmax(prob, dim=1)\n",
    "    argsort = torch.argsort(prob, dim=1, descending=True)\n",
    "    preds.append(pred)\n",
    "    gts.append(gt)    \n",
    "    inp, out = generate_visualization(img, pred)\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    fig.set_size_inches(17,9)\n",
    "    axs[0].imshow(inp)\n",
    "    axs[0].set_title('Groundtruth: '+ labels[int(gt)] )\n",
    "    title = ''\n",
    "    for arg in argsort.squeeze(0)[:2]:\n",
    "        title = title + ' ' + labels[int(arg)]+', Pr: {:.2%}'.format((prob[0][arg]))\n",
    "\n",
    "    axs[1].set_title('Predicted: '+ title)\n",
    "    axs[1].imshow(out)\n",
    "    \n",
    "    for ax in axs:\n",
    "        # Hide grid lines\n",
    "        ax.grid(False)\n",
    "        # Hide axes ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    filepath = os.path.join(output, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = np.asarray([int(_) for _ in gts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.asarray([int(_) for _ in preds ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(gts==preds)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

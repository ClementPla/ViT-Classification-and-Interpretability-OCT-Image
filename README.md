# Vision Transformer For Opthalmic Images



This repo contains the source code for a paper submitted to MICCAI21 (under review).

It is an application of Vision Transformer to medical images, including a comparative study on the explainability heatmaps generated by:

- Deep Taylor Decomposition on ResNet152
- Attention-Rollout from ViT
- Layerwise-Relevance-Propagation from ViT

The trained models can be found in the following drive:



## Dependencies

We rely on various implementations:

- For experiments managements: [nntools](https://github.com/ClementPla/NNTools/tree/master)

- For ViT and DeiT  models : [timm](https://github.com/rwightman/pytorch-image-models)
- For Tokens-2-Token ViT, the authors' implementation: [T2T-ViT](https://github.com/yitu-opensource/T2T-ViT?utm_source=catalyzex.com)
- Transformer Interpretability beyond Attention Visualization (LRP-ViT): [Transformer-explainability](https://github.com/hila-chefer/Transformer-Explainability) 